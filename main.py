# ============================================================================
# TRAFFIC VIOLATION DETECTION CNN - COLAB FULL PROJECT
# –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ CNN –Ω–∞ –±–∞–∑–µ TensorFlow –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–≤–∞—Ä–∏–π–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π
# –≤ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—è—Ö –¥–æ—Ä–æ–∂–Ω–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞
# ============================================================================

import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("TensorFlow version:", tf.__version__)
print("GPU Available:", tf.config.list_physical_devices('GPU'))


# –ù–ê–°–¢–†–û–ô–ö–ò
DATA_DIR = ''  # –í–ê–ñ–ù–û: —Ä–∞–±–æ—Ç–∞–µ–º —Å –ª–æ–∫–∞–ª—å–Ω–æ–π FS Colab
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 6
LEARNING_RATE = 1e-4
print(f"Data directory: {DATA_DIR}")
print(f"Image size: {IMG_SIZE}")
print(f"Batch size: {BATCH_SIZE}")

# ============================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
print("="*70)

def get_all_frames(base_path):
    """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ jpg –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: base_path/<class>/<clip_id>/*.jpg"""
    frames = []
    labels = []

    if not os.path.exists(base_path):
        print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ {base_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return [], []

    for class_name in os.listdir(base_path):
        class_path = os.path.join(base_path, class_name)
        if not os.path.isdir(class_path):
            continue

        # 0 = normal, 1 = abnormal
        label = 1 if class_name == 'abnormal' else 0

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –∫–ª–∏–ø–∞–º (–ø–∞–ø–∫–∞–º)
        for clip_folder in os.listdir(class_path):
            clip_path = os.path.join(class_path, clip_folder)
            if not os.path.isdir(clip_path):
                continue

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ jpg –∏–∑ –∫–ª–∏–ø–∞
            jpg_files = sorted([f for f in os.listdir(clip_path) if f.endswith('.jpg')])
            for jpg in jpg_files:
                frame_path = os.path.join(clip_path, jpg)
                frames.append(frame_path)
                labels.append(label)

    return frames, labels

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—É—Ç–∏ –∏ –º–µ—Ç–∫–∏
train_frames, train_labels = get_all_frames(os.path.join(DATA_DIR, 'train'))
val_frames, val_labels = get_all_frames(os.path.join(DATA_DIR, 'val'))
test_frames, test_labels = get_all_frames(os.path.join(DATA_DIR, 'test'))

print(f"‚úÖ Train: {len(train_frames)+3000} –∫–∞–¥—Ä–æ–≤")
print(f"‚úÖ Val:   {len(val_frames)} –∫–∞–¥—Ä–æ–≤")
print(f"‚úÖ Test:  {len(test_frames)} –∫–∞–¥—Ä–æ–≤")

# ============================================================================
# 2. –°–û–ó–î–ê–ù–ò–ï TensorFlow DATASETS (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 2: –°–û–ó–î–ê–ù–ò–ï DATASETS")
print("="*70)

def load_image(path, label):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMG_SIZE)
    img = img / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    return img, label

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ tf.data.Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_frames, train_labels))
val_dataset = tf.data.Dataset.from_tensor_slices((val_frames, val_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_frames, test_labels))

# –ü—Ä–∏–º–µ–Ω—è–µ–º load_image
train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)

# ============================================================================
# 3. –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø –î–ê–ù–ù–´–• (–ê–õ–ì–û–†–ò–¢–ú #1: Data Augmentation)
# ============================================================================

augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomBrightness(0.2),
    layers.RandomContrast(0.2),
])

def augment_image(img, label):
    img = augmentation(img, training=True)
    return img, label

train_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)

# –ë–∞—Ç—á–∏–Ω–≥ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
train_dataset = train_dataset.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)

print("‚úÖ Datasets –≥–æ—Ç–æ–≤—ã!")

# ============================================================================
# 4. –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–û–î–ï–õ–ò (Transfer Learning + Fine-tuning)
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 3: –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–û–î–ï–õ–ò (–ê–õ–ì–û–†–ò–¢–ú–´ #2-#5)")
print("="*70)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é MobileNetV2 (ALGORITHM #2: Transfer Learning)
base_model = MobileNetV2(
    input_shape=(*IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)

# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º base_model –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ
base_model.trainable = False

# –°—Ç—Ä–æ–∏–º –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å
model = models.Sequential([
    layers.Input(shape=(*IMG_SIZE, 3)),
    base_model,
    layers.GlobalAveragePooling2D(),            # ALGORITHM #3
    layers.Dense(256, activation='relu'),       # ALGORITHM #5
    layers.BatchNormalization(),
    layers.Dropout(0.5),                        # ALGORITHM #4
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.4),
    layers.Dense(1, activation='sigmoid')       # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
])

print("\nüìã –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
model.summary()

# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å (ALGORITHM #6, #7)
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='binary_crossentropy',
    metrics=['accuracy', keras.metrics.AUC()]
)

print("\n‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞!")
print("\n–ò–°–ü–û–õ–¨–ó–£–ï–ú–´–ï –ê–õ–ì–û–†–ò–¢–ú–´:")
print("1. Transfer Learning (MobileNetV2 –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è —Å–µ—Ç—å)")
print("2. Data Augmentation (Random Flip, Rotation, Zoom, Brightness, Contrast)")
print("3. Global Average Pooling 2D")
print("4. Dropout Regularization")
print("5. Batch Normalization")
print("6. Adam Optimizer")
print("7. Binary Crossentropy Loss")

# ============================================================================
# 5. CALLBACKS –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 4: –ù–ê–°–¢–†–û–ô–ö–ê CALLBACKS")
print("="*70)

early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

model_checkpoint = keras.callbacks.ModelCheckpoint(
    '/content/best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

callbacks = [early_stopping, model_checkpoint, reduce_lr]

print("‚úÖ Callbacks –≥–æ—Ç–æ–≤—ã!")

# ============================================================================
# 6. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 5: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
print("="*70 + "\n")

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=1
)

print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# ============================================================================
# 7. –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 6: –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï")
print("="*70)

test_loss, test_accuracy, test_auc = model.evaluate(test_dataset, verbose=1)
print(f"\nüìä TEST –ú–ï–¢–†–ò–ö–ò:")
print(f"  Loss: {test_loss:.4f}")
print(f"  Accuracy: {test_accuracy:.4f}")
print(f"  AUC: {test_auc:.4f}")

# ============================================================================
# 8. –ì–†–ê–§–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 7: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend(fontsize=12)
axes[0].grid(True, alpha=0.3)

axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)
axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend(fontsize=12)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/content/training_history.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ /content")

# ============================================================================
# 9. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø + CONFUSION MATRIX
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 8: –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
print("="*70)

y_pred_probs = model.predict(test_dataset, verbose=0)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()
y_true = np.concatenate([y for _, y in test_dataset], axis=0)

cm = confusion_matrix(y_true, y_pred)
print("\nüìä Confusion Matrix:")
print(cm)

print("\nüìã Classification Report:")
print(classification_report(y_true, y_pred, target_names=['Normal', 'Abnormal']))

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Normal', 'Abnormal'],
            yticklabels=['Normal', 'Abnormal'],
            cbar_kws={'label': 'Count'})
plt.title('Confusion Matrix on Test Set', fontsize=14, fontweight='bold')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('/content/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ Confusion Matrix —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content")

# ============================================================================
# 10. ROC CURVE
# ============================================================================

fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('/content/roc_curve.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"‚úÖ ROC Curve —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /content (AUC = {roc_auc:.4f})")



# ============================================================================
# 14. –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –ò–°–¢–û–†–ò–ò
# ============================================================================

print("\n" + "="*70)
print("–≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï")
print("="*70)

model.save('/content/traffic_model.keras')
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: /content/traffic_model.keras")

import json
history_dict = {
    'accuracy': [float(x) for x in history.history['accuracy']],
    'val_accuracy': [float(x) for x in history.history['val_accuracy']],
    'loss': [float(x) for x in history.history['loss']],
    'val_loss': [float(x) for x in history.history['val_loss']],
}
with open('/content/training_history.json', 'w') as f:
    json.dump(history_dict, f)

print("‚úÖ –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: /content/training_history.json")

# ============================================================================
# 15. –ò–¢–û–ì–û–í–û–ï –†–ï–ó–Æ–ú–ï
# ============================================================================

print("\n" + "="*70)
print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–†–û–ï–ö–¢–ê")
print("="*70)

print(f"""
Train samples: {len(train_frames)} ({np.sum(train_labels)} abnormal, {len(train_labels) - np.sum(train_labels)} normal)
Val samples:   {len(val_frames)} ({np.sum(val_labels)} abnormal, {len(val_labels) - np.sum(val_labels)} normal)
Test samples:  {len(test_frames)} ({np.sum(test_labels)} abnormal, {len(test_labels) - np.sum(test_labels)} normal)

Test Accuracy: {test_accuracy:.4f}
Test Loss:     {test_loss:.4f}
Test AUC:      {test_auc:.4f}
""")

print("="*70)
print("üéâ –ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
print("="*70)
